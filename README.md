# 60 Days 60 AI Agents

Ein modulares Python-Framework zur Entwicklung spezialisierter AI Agents.

## ğŸš€ Schnellstart

1. **Setup der Entwicklungsumgebung:**
   ```bash
   cd main
   ./setup.sh
   ```

2. **Environment aktivieren:**
   ```bash
   source venv/bin/activate
   ```

## ğŸ¤– VerfÃ¼gbare Agents

### Obsidian Vault Agent
Ein intelligenter Agent zur Durchsuchung und Analyse von Obsidian Vaults.

**Features:**
- ğŸ“– Durchsuchen von Notizen nach Inhalt und Titel
- ğŸ“Š Automatische Zusammenfassungen
- ğŸ”— Verbindungsanalyse zwischen Notizen
- ğŸ“ˆ Vault-Statistiken
- ğŸ’¬ Interaktiver CLI-Chatbot

**Setup:**
1. Konfiguration anpassen:
   ```bash
   cd obsidian_agent
   cp config_example.yaml config.yaml
   # Bearbeite config.yaml und setze deinen Vault-Pfad
   ```

2. Agent starten:
   ```bash
   python cli_chatbot.py
   ```

**UnterstÃ¼tzte LLM-Provider:**
- ğŸ¦™ **Ollama** (empfohlen fÃ¼r lokale Entwicklung)
- ğŸ¤– **OpenAI** (GPT-3.5/GPT-4)
- ğŸ§  **Claude** (Anthropic)

## ğŸ“ Projektstruktur

```
main/
â”œâ”€â”€ venv/                    # Globales Virtual Environment
â”œâ”€â”€ requirements.txt         # Python Dependencies
â”œâ”€â”€ setup.sh                # Setup-Skript
â”œâ”€â”€ .gitignore              # Git-Konfiguration
â””â”€â”€ obsidian_agent/         # Obsidian Vault Agent
    â”œâ”€â”€ config_example.yaml  # Beispiel-Konfiguration
    â”œâ”€â”€ config.yaml         # Deine Konfiguration (wird ignoriert)
    â”œâ”€â”€ config_manager.py    # Konfiguration-Management
    â”œâ”€â”€ vault_reader.py      # Obsidian Vault Parser
    â”œâ”€â”€ llm_manager.py       # LLM Provider Management
    â”œâ”€â”€ obsidian_agent.py    # Haupt-Agent Klasse
    â””â”€â”€ cli_chatbot.py       # CLI Interface
```

## ğŸ›  Entwicklung

### Neuen Agent hinzufÃ¼gen

1. Erstelle einen neuen Ordner fÃ¼r deinen Agent
2. Implementiere die Basis-Klassen:
   - `config_manager.py` - Konfiguration
   - `llm_manager.py` - LLM-Anbindung (kann importiert werden)
   - `your_agent.py` - Agent-Logik
   - `cli_interface.py` - Benutzerinterface

### LLM-Provider konfigurieren

**Ollama (lokal):**
```bash
# Ollama installieren und Modell herunterladen
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3.1
```

**OpenAI:**
```yaml
llm:
  provider: "openai"
  openai:
    api_key: "sk-your-api-key"
    model: "gpt-3.5-turbo"
```

**Claude:**
```yaml
llm:
  provider: "claude"
  claude:
    api_key: "your-claude-api-key"
    model: "claude-3-sonnet-20240229"
```

## ğŸ“ Lizenz

MIT License - siehe LICENSE Datei fÃ¼r Details.
